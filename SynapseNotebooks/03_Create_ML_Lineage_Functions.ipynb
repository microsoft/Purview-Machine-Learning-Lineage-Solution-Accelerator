{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Copyright (c) Microsoft. All rights reserved.\r\n",
        "\r\n",
        "Licensed under the MIT license."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_entity_details(qualifiedName,typeName):\n",
        "    entities = client.get_entity(\n",
        "        qualifiedName=[qualifiedName],\n",
        "        typeName=typeName\n",
        "    )\n",
        "    for entity in entities.get(\"entities\"):\n",
        "        entity = entity\n",
        "        break\n",
        "    return entity\n",
        "#get_entity_details('https://sampledataadls.dfs.core.windows.net/masterdata/employees.csv','azure_datalake_gen2_path')\n",
        "\n",
        "def get_entity_guid(qualifiedName,typeName):\n",
        "    entities = client.get_entity(\n",
        "        qualifiedName=[qualifiedName],\n",
        "        typeName=typeName\n",
        "    )\n",
        "    for entity in entities.get(\"entities\"):\n",
        "        entity_guid = entity.get(\"guid\")\n",
        "        break\n",
        "    return entity_guid\n",
        "#get_entity_guid('https://sampledataadls.dfs.core.windows.net/creditriskdata/borrower.csv','azure_datalake_gen2_path')\n",
        "\n",
        "def get_entity_schema(guid):\n",
        "    columns = []\n",
        "    results = client.get_entity(guid)\n",
        "    for entity in results[\"entities\"]:\n",
        "        if \"tabular_schema\" in entity[\"relationshipAttributes\"]:\n",
        "            ts = entity[\"relationshipAttributes\"][\"tabular_schema\"]\n",
        "            ts_entity = client.get_entity(ts[\"guid\"])\n",
        "            for schema in ts_entity[\"entities\"]:\n",
        "                for col in schema[\"relationshipAttributes\"][\"columns\"]:\n",
        "                    if col['displayText'] != ':csv':\n",
        "                        columns.append(col['displayText'])\n",
        "    return(columns)\n",
        "    \n",
        "# ent_guid = 'a8698a33-9174-43cb-8835-26968862e2bf'\n",
        "# get_entity_schema(ent_guid)\n",
        "\n",
        "def create_data_entity_with_schema_and_parent(df_data,entityname,entitytype='custom_ml_dataset',parent_entityname=None,parent_entitytype='custom_ml_datastore'):\n",
        "    # Create an asset for the output data schema.\n",
        "    output_schema_entity = AtlasEntity(\n",
        "    name=\"schema-\" + entityname,\n",
        "    qualified_name = \"pyapacheatlas://\"+\"schema-\" + entityname,\n",
        "    typeName=\"tabular_schema\",\n",
        "    guid=guid.get_guid()\n",
        "    )\n",
        "\n",
        "    df_data_schema = pd.DataFrame(list(zip(list(df_data.columns), list(df_data.dtypes))),columns=['column','dtype'])\n",
        "\n",
        "    #Iterate over the out data frame's columns and create entities\n",
        "    output_entity_schema_columns = []\n",
        "    #for column in df.schema:\n",
        "    for index, row in df_data_schema.iterrows():  \n",
        "        temp_column = AtlasEntity(\n",
        "            name = row.column,\n",
        "            typeName = \"column\",\n",
        "            qualified_name = \"pyapacheatlas://schema-\" + entityname + \"#\" + row.column,\n",
        "            guid=guid.get_guid(),\n",
        "            attributes = {\"type\":str(row.dtype),\"description\": row.column},\n",
        "            relationshipAttributes = {\"composeSchema\":output_schema_entity.to_json(minimum=True)}\n",
        "        )\n",
        "        output_entity_schema_columns.append(temp_column)\n",
        "\n",
        "\n",
        "    if parent_entityname:\n",
        "        dstore_entity = get_entity_details(\"pyapacheatlas://\"+parent_entityname, parent_entitytype)\n",
        "        # Create a entity for dataset \n",
        "        dataset_output_entity = AtlasEntity(\n",
        "            name=entityname,\n",
        "            typeName=entitytype,\n",
        "            qualified_name=\"pyapacheatlas://\" + entityname,\n",
        "            guid = guid.get_guid(),\n",
        "            relationshipAttributes = {\n",
        "                \"tabular_schema\": output_schema_entity.to_json(minimum=True),\n",
        "                \"datastore\":dstore_entity\n",
        "            }\n",
        "        )\n",
        "    else:\n",
        "        # Create a entity for dataset \n",
        "        dataset_output_entity = AtlasEntity(\n",
        "            name=entityname,\n",
        "            typeName=entitytype,\n",
        "            qualified_name=\"pyapacheatlas://\" + entityname,\n",
        "            guid = guid.get_guid(),\n",
        "            relationshipAttributes = {\n",
        "                \"tabular_schema\": output_schema_entity.to_json(minimum=True)\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # Prepare all the entities as a batch to be uploaded.\n",
        "    batch = [dataset_output_entity, output_schema_entity] + output_entity_schema_columns\n",
        "    batch\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=batch)\n",
        "    \n",
        "def create_data_entity_with_schema(df_data,entityname,entitytype='custom_ml_dataset'):\n",
        "    # Create an asset for the output data schema.\n",
        "    output_schema_entity = AtlasEntity(\n",
        "    name=\"schema-\" + entityname,\n",
        "    qualified_name = \"pyapacheatlas://\"+\"schema-\" + entityname,\n",
        "    typeName=\"tabular_schema\",\n",
        "    guid=guid.get_guid()\n",
        "    )\n",
        "\n",
        "    df_data_schema = pd.DataFrame(list(zip(list(df_data.columns), list(df_data.dtypes))),columns=['column','dtype'])\n",
        "\n",
        "    #Iterate over the out data frame's columns and create entities\n",
        "    output_entity_schema_columns = []\n",
        "    #for column in df.schema:\n",
        "    for index, row in df_data_schema.iterrows():  \n",
        "        temp_column = AtlasEntity(\n",
        "            name = row.column,\n",
        "            typeName = \"column\",\n",
        "            qualified_name = \"pyapacheatlas://schema-\" + entityname + \"#\" + row.column,\n",
        "            guid=guid.get_guid(),\n",
        "            attributes = {\"type\":str(row.dtype),\"description\": row.column},\n",
        "            relationshipAttributes = {\"composeSchema\":output_schema_entity.to_json(minimum=True)}\n",
        "        )\n",
        "        output_entity_schema_columns.append(temp_column)\n",
        "\n",
        "    # Create a entity for dataset \n",
        "    dataset_output_entity = AtlasEntity(\n",
        "        name=entityname,\n",
        "        typeName=entitytype,\n",
        "        qualified_name=\"pyapacheatlas://\" + entityname,\n",
        "        guid = guid.get_guid(),\n",
        "        relationshipAttributes = {\n",
        "            \"tabular_schema\": output_schema_entity.to_json(minimum=True)\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Prepare all the entities as a batch to be uploaded.\n",
        "    batch = [dataset_output_entity, output_schema_entity] + output_entity_schema_columns\n",
        "    batch\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=batch)\n",
        "    \n",
        "def create_lineage_for_entities(experimentname,processname,in_ent_qns,out_ent_qns,process_type_name='Process',ColumnMapping=False):\n",
        "    # create a process \n",
        "    # inputs: list of (entity,type) tuples\n",
        "    # outputs: list of (entity,type) tuples\n",
        "\n",
        "    from pyapacheatlas.core import AtlasProcess\n",
        "\n",
        "    in_ent_guids = []\n",
        "    for in_ent_qn in in_ent_qns:\n",
        "        #print(in_ent_qn,in_ent_qns[in_ent_qn])\n",
        "        in_ent_guid = get_entity_guid(in_ent_qn,in_ent_qns[in_ent_qn])\n",
        "        in_ent_guids.append({'guid':in_ent_guid})\n",
        "    \n",
        "    out_ent_guids = []\n",
        "    for out_ent_qn in out_ent_qns:\n",
        "        #print(in_ent_qn,in_ent_qns[in_ent_qn])\n",
        "        out_ent_guid = get_entity_guid(out_ent_qn,out_ent_qns[out_ent_qn])\n",
        "        out_ent_guids.append({'guid':out_ent_guid})\n",
        "\n",
        "    process_name = experimentname + processname\n",
        "    process_qn = \"pyapacheatlas://\" + process_name\n",
        "\n",
        "    if ColumnMapping == False:\n",
        "        process_type_name = process_type_name\n",
        "\n",
        "        process = AtlasProcess(\n",
        "            name=process_name,\n",
        "            typeName=process_type_name,\n",
        "            qualified_name=process_qn,\n",
        "            inputs = in_ent_guids,\n",
        "            outputs = out_ent_guids,\n",
        "            guid=guid.get_guid()\n",
        "        )\n",
        "    else:\n",
        "        process_type_name = \"ProcessWithColumnMapping\"\n",
        "\n",
        "        column_mapping_attributes = []\n",
        "        for in_ent_qn in in_ent_qns:\n",
        "            cl_mapping = []\n",
        "            in_ent_columns = get_entity_schema(get_entity_guid(in_ent_qn,in_ent_qns[in_ent_qn]))\n",
        "            for in_col in in_ent_columns:\n",
        "                cl_mapping.append({\"Source\":in_col,\"Sink\":in_col})\n",
        "                #break\n",
        "            mapping = {\n",
        "            'DatasetMapping': {'Source':in_ent_qn,'Sink':list(out_ent_qns.keys())[0]},\n",
        "            'ColumnMapping': cl_mapping\n",
        "            }\n",
        "            column_mapping_attributes.append(mapping)\n",
        "\n",
        "        process = AtlasProcess(\n",
        "            name=process_name,\n",
        "            typeName=process_type_name,\n",
        "            qualified_name=process_qn,\n",
        "            inputs = in_ent_guids,\n",
        "            outputs = out_ent_guids,\n",
        "            guid=guid.get_guid(),\n",
        "            attributes={\"columnMapping\":json.dumps(column_mapping_attributes)}\n",
        "        )\n",
        "\n",
        "    # Prepare all the entities as a batch to be uploaded.\n",
        "    batch = [process]\n",
        "    batch\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=batch)\n",
        "    \n",
        "def create_entity(name,typeName,config_attibutes):\n",
        "    # Create an entity\n",
        "    name = name \n",
        "    qn = \"pyapacheatlas://\" + name\n",
        "\n",
        "    exp_config_entity = AtlasEntity(\n",
        "        name=name,\n",
        "        typeName=typeName,\n",
        "        qualified_name=qn,\n",
        "        guid = guid.get_guid(),\n",
        "        attributes = config_attibutes\n",
        "    )\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=[exp_config_entity.to_json()])\n",
        "\n",
        "        \n",
        "def get_dataset_details(indataset,experiment_name=''):\n",
        "    result = []\n",
        "    #print(indataset)\n",
        "    if 'FileDataset' in str(type((indataset))):\n",
        "        dssource = eval(json.loads(str(indataset).replace('FileDataset',''))['source'][0])\n",
        "        sourcestore = dssource[0]\n",
        "        sourcepath = dssource[1]\n",
        "        sourcepathfiles = indataset.to_path()\n",
        "        for sourcepathfile in sourcepathfiles:\n",
        "            entityname = sourcepath.split('/')[-1] + sourcepathfile.replace('/','_') #.replace('.parquet','').replace('.csv','')\n",
        "            #print('\\nFileDataset:',entityname)\n",
        "\n",
        "            dsdatastore = Datastore.get(ws, sourcestore)\n",
        "            datastore_path = [DataPath(dsdatastore, sourcepath+sourcepathfile.replace('/',''))]\n",
        "   \n",
        "            if '.parquet' in sourcepathfile:\n",
        "                tabular_dataset = Dataset.Tabular.from_parquet_files(path=datastore_path)\n",
        "                df_data = tabular_dataset.take(10).to_pandas_dataframe()\n",
        "                \n",
        "            elif '.csv' in sourcepathfile:\n",
        "                tabular_dataset = Dataset.Tabular.from_delimited_files(path=datastore_path,encoding ='iso88591') \n",
        "                #'utf8', 'iso88591', 'latin1', 'ascii', 'utf16', 'utf32', 'utf8bom' and 'windows1252'\n",
        "                df_data = tabular_dataset.take(10).to_pandas_dataframe()\n",
        "            \n",
        "            if experiment_name != '':\n",
        "                result.append((entityname + '_' + experiment_name,df_data))\n",
        "            else:\n",
        "                result.append((entityname,df_data))\n",
        "\n",
        "    elif 'TabularDataset' in str(type((indataset))):\n",
        "        tabular_dataset = indataset\n",
        "        entityname = json.loads(str(indataset).replace('TabularDataset',''))['registration']['name']\n",
        "        \n",
        "        # dataset = Dataset.get_by_name(ws, name=entityname)\n",
        "        # try:\n",
        "        #     sourcestore = json.loads(dataset._definition)['blocks'][0]['arguments']['datastore']['datastoreName']\n",
        "        # except:\n",
        "        #     sourcestore = json.loads(dataset._definition)['blocks'][0]['arguments']['datastores'][0]['datastoreName']\n",
        "        df_data = tabular_dataset.take(10).to_pandas_dataframe()\n",
        "        #print('TabularDataset:', entityname)\n",
        "        result.append((entityname,df_data))\n",
        "    return result\n",
        "\n",
        "\n",
        "from azureml.core import Experiment\n",
        "from azureml.pipeline.core import PipelineRun\n",
        "\n",
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "from azureml.data.datapath import DataPath\n",
        "import json  \n",
        "import pandas as pd\n",
        "\n",
        "def create_aml_experiment_steps(ws,experiment_name):\n",
        "    experiments_lst = Experiment.list(ws)\n",
        "    for experiment in experiments_lst:\n",
        "        if experiment.name == experiment_name:\n",
        "            #print(experiment)\n",
        "            exp = Experiment(ws,experiment.name)\n",
        "            for run in exp.get_runs(): \n",
        "                rundetails = run.get_details()\n",
        "                #print(rundetails)\n",
        "                if rundetails['status'] != 'Completed': #continue until we find a completed run \n",
        "                    continue\n",
        "                pipeline_run = PipelineRun(exp, rundetails['runId'])\n",
        "\n",
        "                steps = pipeline_run.get_steps()\n",
        "                for step_run in steps:\n",
        "                    step_run_details = step_run.get_details_with_logs()\n",
        "                    #print(step_run_details)\n",
        "                    #print(step_run_details['runDefinition']['script'])\n",
        "\n",
        "                    purview_basepath = 'pyapacheatlas://'\n",
        "                    in_ent_qns = {}\n",
        "                    out_ent_qns = {}\n",
        "                    #print(step_run_details)\n",
        "                    step_name = step_run.name #step_run_details['runDefinition']['script']\n",
        "                    #print(step_name)\n",
        "                    \n",
        "                    #print('\\n Input Datasets:\\n')\n",
        "                    for indataset in step_run_details['inputDatasets']:\n",
        "                        in_result = get_dataset_details(indataset['dataset'],experiment_name)\n",
        "                        #print(in_result)\n",
        "                        #create entities                        \n",
        "                        for in_res in in_result:\n",
        "                            data_ent_name = in_res[0].strip('_')\n",
        "                            create_data_entity_with_schema(in_res[1],data_ent_name,'custom_ml_dataset')\n",
        "                            in_ent_qns[purview_basepath + data_ent_name] = 'custom_ml_dataset'\n",
        "                        #break\n",
        "                    #print('\\n Output Datasets:\\n')\n",
        "                    for outdataset in step_run_details['outputDatasets']:\n",
        "                        out_result = get_dataset_details(outdataset['dataset'],experiment_name)\n",
        "                        #print(out_result)\n",
        "                        #create entities\n",
        "                        for out_res in out_result:\n",
        "                            data_ent_name = out_res[0].strip('_')\n",
        "                            create_data_entity_with_schema(out_res[1],data_ent_name,'custom_ml_dataset')\n",
        "                            out_ent_qns[purview_basepath + data_ent_name] = 'custom_ml_dataset'\n",
        "                        #break\n",
        "                    #print(in_ent_qns,out_ent_qns)\n",
        "                    create_lineage_for_entities(experiment_name + '_',step_name, in_ent_qns,out_ent_qns,process_type_name='custom_ml_experiment_step',ColumnMapping=False)\n",
        "                    #break    \n",
        "                \n",
        "                break # break after processing one completed run\n",
        "            break #after finding the experiment\n",
        "\n",
        "\n",
        "#create workspace entity\n",
        "def create_workspace_entities(ws):\n",
        "\n",
        "    config_attibutes={}\n",
        "    temp_column={}\n",
        "\n",
        "    temp_column['name'] = ws.name\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['subscription_id'] = ws.subscription_id\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['resource_group'] = ws.resource_group\n",
        "    config_attibutes.update(temp_column)\n",
        "\n",
        "    create_entity(ws.name,'custom_ml_workspace',config_attibutes)\n",
        "    #break\n",
        "\n",
        "\n",
        "#create all datastore entities\n",
        "def create_datastore_entities(ws):\n",
        "    for datastore in ws.datastores.values():\n",
        "        config_attibutes={}\n",
        "        temp_column={}\n",
        "        \n",
        "        temp_column['name'] = datastore.name\n",
        "        config_attibutes.update(temp_column)\n",
        "\n",
        "        if ('AzureDataLakeGen2Datastore' in str(type(datastore))) or ('AzureBlobDatastore' in str(type(datastore))):\n",
        "            temp_column['container_name'] = datastore.container_name\n",
        "            config_attibutes.update(temp_column)\n",
        "            temp_column['account_name'] = datastore.account_name\n",
        "            config_attibutes.update(temp_column)\n",
        "            temp_column['protocol'] = datastore.protocol\n",
        "            config_attibutes.update(temp_column)\n",
        "            temp_column['endpoint'] = datastore.endpoint\n",
        "            config_attibutes.update(temp_column)\n",
        "        elif 'AzureSqlDatabaseDatastore' in str(type(datastore)):\n",
        "            #print('sql',datastore.server_name)\n",
        "            temp_column['server_name'] = datastore.server_name\n",
        "            config_attibutes.update(temp_column)\n",
        "            temp_column['database_name'] = datastore.database_name\n",
        "            config_attibutes.update(temp_column)\n",
        "        elif 'AzureBlobDatastore' in str(type(datastore)):    \n",
        "            pass\n",
        "\n",
        "        create_entity(datastore.name,'custom_ml_datastore',config_attibutes)\n",
        "        #break\n",
        "\n",
        "    #create workspace and datastore relationship\n",
        "    purview_basepath = 'pyapacheatlas://'\n",
        "    for datastore in ws.datastores.values():\n",
        "        relationshiptype = 'custom_ml_workspace_datastore'\n",
        "        end1type = 'custom_ml_workspace'\n",
        "        end2type = 'custom_ml_datastore'\n",
        "        end1_qn = purview_basepath + ws.name\n",
        "        end2_qn = purview_basepath + datastore.name\n",
        "        try:\n",
        "            create_entities_relationship(relationshiptype,end1type,end2type,end1_qn,end2_qn)\n",
        "        except:\n",
        "            pass # ignore if relationship exists\n",
        "\n",
        "#create all dataset entities (with datastore as parent)\n",
        "from azureml.core import Workspace, Datastore, Dataset\n",
        "import pandas as pd\n",
        "def create_dataset_entities(ws,parent_flag=True):\n",
        "    purview_basepath = 'pyapacheatlas://'\n",
        "    for dsname in ws.datasets:\n",
        "        dataset = ws.datasets[dsname]\n",
        "        try:\n",
        "            if 'FileDataset' in str(type((dataset))):\n",
        "                datasetsource = eval(json.loads(str(dataset).replace('FileDataset',''))['source'][0])[0]\n",
        "            elif 'TabularDataset' in str(type((dataset))):\n",
        "                datasetsource = eval(json.loads(str(dataset).replace('TabularDataset',''))['source'][0])[0]\n",
        "            dsdetails = get_dataset_details(dataset)\n",
        "            #print(dsdetails)\n",
        "            for ds in dsdetails:\n",
        "                if parent_flag == False:\n",
        "                    create_data_entity_with_schema(ds[1],dsname,'custom_ml_dataset')\n",
        "                    create_lineage_for_entities('',('register_' + dsname), {(purview_basepath+datasetsource):'custom_ml_datastore'},\n",
        "                                                {(purview_basepath+ds[0]):'custom_ml_dataset'},ColumnMapping=False)\n",
        "                else:                    \n",
        "                    create_data_entity_with_schema_and_parent(ds[1],dsname,entitytype='custom_ml_dataset',\n",
        "                                                              parent_entityname=datasetsource,parent_entitytype='custom_ml_datastore')    \n",
        "        except:\n",
        "            print('Error:',dsname)        \n",
        "        #break\n",
        "        \n",
        "        \n",
        "#create experiment entity\n",
        "from azureml.core import Experiment\n",
        "\n",
        "def create_experiment_entities(ws):\n",
        "    for experiment in Experiment.list(ws):\n",
        "        #create experiment entity\n",
        "        config_attibutes={}\n",
        "        temp_column={}\n",
        "\n",
        "        temp_column['name'] = experiment.name\n",
        "        config_attibutes.update(temp_column)\n",
        "\n",
        "        create_entity(experiment.name,'custom_ml_experiment',config_attibutes)\n",
        "        #break\n",
        "        \n",
        "        purview_basepath = 'pyapacheatlas://'\n",
        "\n",
        "        #create experiment relationship to workspace\n",
        "        relationshiptype = 'custom_ml_workspace_experiment'\n",
        "        end1type = 'custom_ml_workspace'\n",
        "        end2type = 'custom_ml_experiment'\n",
        "        end1_qn = purview_basepath + ws.name\n",
        "        end2_qn = purview_basepath + experiment.name\n",
        "        try:\n",
        "            create_entities_relationship(relationshiptype,end1type,end2type,end1_qn,end2_qn)\n",
        "        except:\n",
        "            pass # ignore if relationship exists\n",
        "        \n",
        "        for run in experiment.get_runs(): \n",
        "            rundetails = run.get_details()\n",
        "            #print(rundetails)\n",
        "            if rundetails['status'] != 'Completed': #continue until we find a completed run \n",
        "                continue\n",
        "            # print(rundetails['properties']['azureml.runsource'])\n",
        "            #create experiment steps\n",
        "            if rundetails['properties']['azureml.runsource'] == 'azureml.PipelineRun':\n",
        "                print(experiment.name)\n",
        "                create_aml_experiment_steps(ws,experiment.name)\n",
        "\n",
        "                pipeline_run = PipelineRun(experiment, rundetails['runId'])\n",
        "\n",
        "                steps = pipeline_run.get_steps()\n",
        "                for step_run in steps:\n",
        "                    #print(experiment.name + '_' + step_run.name)\n",
        "                    \n",
        "                    #create experiment relationship to workspace\n",
        "                    relationshiptype = 'custom_ml_experiment_to_experimentstep'\n",
        "                    end1type = 'custom_ml_experiment'\n",
        "                    end2type = 'custom_ml_experiment_step'\n",
        "                    end1_qn = purview_basepath + experiment.name\n",
        "                    end2_qn = purview_basepath + experiment.name + '_' + step_run.name\n",
        "                    try:\n",
        "                        create_entities_relationship(relationshiptype,end1type,end2type,end1_qn,end2_qn)\n",
        "                    except:\n",
        "                        pass # ignore if relationship exists\n",
        "\n",
        "            break # break after processing one completed run\n",
        "        #break\n",
        "\n",
        "def create_entities_relationship(relationshiptype,end1type,end2type,end1_qn,end2_qn):\n",
        "    relationship = {}\n",
        "    end1 = {}\n",
        "    end2 = {}\n",
        "\n",
        "    end1[\"guid\"] = get_entity_guid(end1_qn,end1type)\n",
        "    end1[\"typeName\"] = end1type\n",
        "    end1[\"uniqueAttributes\"] = {\"qualifiedName\": end1_qn}\n",
        "\n",
        "    end2[\"guid\"] = get_entity_guid(end2_qn,end2type)\n",
        "    end2[\"typeName\"] = end2type\n",
        "    end2[\"uniqueAttributes\"] = {\"qualifiedName\": end2_qn}\n",
        "\n",
        "    relationship[\"typeName\"] = relationshiptype\n",
        "    relationship[\"attributes\"] = {}\n",
        "    relationship[\"guid\"] = guid.get_guid()\n",
        "    relationship[\"provenanceType\"] = 0\n",
        "    relationship[\"end1\"] = end1\n",
        "    relationship[\"end2\"] = end2\n",
        "    relationship\n",
        "    \n",
        "    client.upload_relationship(relationship)         \n",
        "       \n",
        "def create_package_entities(experimentname,packageslist):\n",
        "    packages_name = experimentname + '-packages' \n",
        "    packages_qn = \"pyapacheatlas://\" + packages_name\n",
        "\n",
        "    # Create an asset for the packages.\n",
        "    packages_entity = AtlasEntity(\n",
        "        name = packages_name,\n",
        "        qualified_name = packages_qn,\n",
        "        typeName=\"custom_ml_packages\",\n",
        "        attributes = {\"notes\":\"test note\"},\n",
        "        guid=guid.get_guid()\n",
        "    )\n",
        "\n",
        "    packages_entity.to_json(minimum=True)\n",
        "\n",
        "    atlas_packages = []\n",
        "    relationships = []\n",
        "    for package in packageslist:\n",
        "        package_attibutes={}\n",
        "        temp_column={}\n",
        "        temp_column['programming_language'] = str(package[0])\n",
        "        package_attibutes.update(temp_column)\n",
        "        temp_column['package_name'] = str(package[1])\n",
        "        package_attibutes.update(temp_column)\n",
        "        temp_column['version'] = str(package[2])\n",
        "        package_attibutes.update(temp_column)\n",
        "        temp_column['notes'] = str(package[3])\n",
        "        package_attibutes.update(temp_column)\n",
        "\n",
        "        # Create an entity for each package\n",
        "        name = str(package[1]) #experimentname + '-package-' + package[1] \n",
        "        qn =   packages_qn + '#' + str(package[1])     #\"pyapacheatlas://\" + name\n",
        "\n",
        "        package_entity = AtlasEntity(\n",
        "            name= name,\n",
        "            typeName=\"custom_ml_package\",\n",
        "            qualified_name=qn,\n",
        "            guid = guid.get_guid(),\n",
        "            attributes = package_attibutes,\n",
        "            relationshipAttributes = {\"packages\":packages_entity.to_json(minimum=True)}\n",
        "        )\n",
        "        atlas_packages.append(package_entity)\n",
        "\n",
        "    atlas_packages\n",
        "\n",
        "    # Prepare all the entities as a batch to be uploaded.\n",
        "    batch = [packages_entity] + atlas_packages\n",
        "    client.upload_entities(batch=batch) \n",
        "    \n",
        "def create_experiment_config_entity(ws,experiment_name,automl_run):\n",
        "    # Get experiment config from AML run\n",
        "    import json\n",
        "    import pandas as pd\n",
        "    run_properties = automl_run.get_properties()\n",
        "    run_properties\n",
        "\n",
        "    AMLSettingsJsonString = run_properties['AMLSettingsJsonString']\n",
        "    AMLSettings = json.loads(AMLSettingsJsonString)\n",
        "\n",
        "    df_config = pd.DataFrame(list(AMLSettings.items()),columns = ['key','value']) \n",
        "\n",
        "    keys = ['task_type','enable_early_stopping','experiment_timeout_minutes','primary_metric','compute_target','label_column_name','n_cross_validations','model_explainability']\n",
        "\n",
        "    df_config = df_config[df_config['key'].isin(keys)]\n",
        "\n",
        "    dict_config = df_config.to_dict(orient = 'records')\n",
        "    dict_config\n",
        "\n",
        "    config_attibutes={}\n",
        "    for attibutes in dict_config:\n",
        "        temp_column={}\n",
        "        temp_column[attibutes['key']] = attibutes['value']\n",
        "        config_attibutes.update(temp_column)\n",
        "    config_attibutes\n",
        "\n",
        "    # Create a entity for exp config \n",
        "    name = experiment_name + \"-config\"\n",
        "    qn = \"pyapacheatlas://\" + name\n",
        "\n",
        "    exp_config_entity = AtlasEntity(\n",
        "        name=name,\n",
        "        typeName=\"custom_ml_exp_config\",\n",
        "        qualified_name=qn,\n",
        "        guid = guid.get_guid(),\n",
        "        attributes = config_attibutes\n",
        "    )\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=[exp_config_entity.to_json()])\n",
        "    \n",
        "def create_model_entity(ws,experiment_name,modelname):\n",
        "    # get deployed model\n",
        "    from azureml.core.model import Model\n",
        "    model = Model(ws, modelname)\n",
        "\n",
        "    config_attibutes={}\n",
        "    temp_column={}\n",
        "    temp_column['workspace_name'] = model.workspace.name\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['workspace_subscription_id'] = model.workspace.subscription_id\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['workspace_subscription_id'] = model.workspace.subscription_id\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['workspace_resource_group'] = model.workspace.resource_group\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['name'] = model.name\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['id'] = model.id\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['version'] = model.version\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['tags'] = model.tags\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['properties'] = model.properties\n",
        "    config_attibutes.update(temp_column)\n",
        "\n",
        "    # Create a entity for Model\n",
        "    name = modelname \n",
        "    qn = \"pyapacheatlas://\" + name\n",
        "\n",
        "    exp_config_entity = AtlasEntity(\n",
        "        name=name,\n",
        "        typeName=\"custom_ml_model\",\n",
        "        qualified_name=qn,\n",
        "        guid = guid.get_guid(),\n",
        "        attributes = config_attibutes\n",
        "    )\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=[exp_config_entity.to_json()])    \n",
        "    \n",
        "def create_model_metrics_entity(experiment_name,best_run):\n",
        "    metrics = best_run.get_metrics()\n",
        "\n",
        "    # select relevant metrics\n",
        "    auc = metrics.get('AUC_weighted')\n",
        "    accuracy = metrics.get('accuracy')\n",
        "    precision = metrics.get('precision_score_weighted')\n",
        "    recall = metrics.get('recall_score_weighted')\n",
        "    f1 = metrics.get('f1_score_weighted')\n",
        "\n",
        "    # # combine into single dataframe\n",
        "    # metrics_df = sc.parallelize([['AUC', auc], ['Accuracy', accuracy], ['Precision', precision], ['Recall', recall], ['F1', f1]]).toDF(('Metric', 'Value'))\n",
        "    metrics = ['AUC','Accuracy','Precision','Recall','F1']\n",
        "    metricslist= [auc,accuracy,precision,recall,f1]\n",
        "    columns = ['Metric','Value']\n",
        "    metrics_df =  pd.DataFrame(zip(metrics, metricslist),columns=columns)\n",
        "\n",
        "\n",
        "    dict_metrics = metrics_df.to_dict(orient = 'records')\n",
        "    dict_metrics\n",
        "\n",
        "    config_attibutes={}\n",
        "    for attibutes in dict_metrics:\n",
        "        temp_column={}\n",
        "        temp_column[attibutes['Metric']] = attibutes['Value']\n",
        "        config_attibutes.update(temp_column)\n",
        "    config_attibutes\n",
        "\n",
        "    name = experiment_name + \"-modelmetrics\"\n",
        "    qn = \"pyapacheatlas://\" + name\n",
        "\n",
        "    # Create a entity for model metrics\n",
        "    exp_config_entity = AtlasEntity(\n",
        "        name=name,\n",
        "        typeName=\"custom_ml_model_metrics\",\n",
        "        qualified_name=qn,\n",
        "        guid = guid.get_guid(),\n",
        "        attributes = config_attibutes\n",
        "    )\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=[exp_config_entity.to_json()])\n",
        "    \n",
        "def create_experiment_lineage(experimentname,exp_data_qn,exp_config_qn,model_metrics_qn,model_qn):        \n",
        "    # create experiment process \n",
        "    # inputs: prepareddata, modelconfig \n",
        "    # outputs: model metrics and registered model\n",
        "\n",
        "    from pyapacheatlas.core import AtlasProcess\n",
        "\n",
        "    in_data_ent_guid = get_entity_guid(exp_data_qn,'custom_dataset')\n",
        "    in_exp_config_guid = get_entity_guid(exp_config_qn,'custom_ml_exp_config')\n",
        "    out_model_metrics_guid = get_entity_guid(model_metrics_qn,'custom_ml_model_metrics')\n",
        "    out_model_guid = get_entity_guid(model_qn,'custom_ml_model')\n",
        "\n",
        "    process_name = experimentname + '-train'\n",
        "    process_qn = \"pyapacheatlas://\" + process_name\n",
        "    process_type_name = \"Process\"\n",
        "\n",
        "    process = AtlasProcess(\n",
        "        name=process_name,\n",
        "        typeName=process_type_name,\n",
        "        qualified_name=process_qn,\n",
        "        inputs = [{\"guid\":in_data_ent_guid},{\"guid\":in_exp_config_guid}],\n",
        "        outputs = [{\"guid\":out_model_metrics_guid},{\"guid\":out_model_guid}],\n",
        "        guid=guid.get_guid()\n",
        "    )\n",
        "\n",
        "    # Prepare all the entities as a batch to be uploaded.\n",
        "    batch = [process]\n",
        "    batch\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=batch)  \n",
        "    \n",
        "def create_model_service_entity(ws,experimentname,aci_service_name,samplejson):\n",
        "    # get deployed ACI Web Service\n",
        "    from azureml.core.webservice import AciWebservice\n",
        "    aciws = AciWebservice(ws, aci_service_name)\n",
        "\n",
        "    config_attibutes={}\n",
        "    temp_column={}\n",
        "    temp_column['workspace_name'] = aciws.workspace.name\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['workspace_subscription_id'] = aciws.workspace.subscription_id\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['workspace_resource_group'] = aciws.workspace.resource_group\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['name'] = aciws.name\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['image_id'] = aciws.image_id\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['compute_type'] = aciws.compute_type\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['state'] = aciws.state\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['scoring_uri'] = aciws.scoring_uri\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['tags'] = aciws.tags\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['state'] = aciws.state\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['properties'] = aciws.properties\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['created_by'] = aciws.created_by\n",
        "    config_attibutes.update(temp_column)\n",
        "    temp_column['sample_json'] = samplejson\n",
        "    config_attibutes.update(temp_column)\n",
        "\n",
        "    name = experimentname + \"-model_endpoint\"\n",
        "    qn = \"pyapacheatlas://\" + name\n",
        "\n",
        "    # Create a entity for ACI Web Service\n",
        "    endpoint_entity = AtlasEntity(\n",
        "        name=name,\n",
        "        typeName=\"custom_ml_model_endpoint\",\n",
        "        qualified_name=qn,\n",
        "        guid = guid.get_guid(),\n",
        "        attributes = config_attibutes\n",
        "    )\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=[endpoint_entity.to_json()])    \n",
        "    \n",
        "def create_powerbi_dataset_and_lineage(experiment_name,pbi_workspace,pbi_datasetid,pbidata_ent_name,ml_dataset_ent_name,ml_dataset_ent_type):\n",
        "    \n",
        "    pbidata_entity_type = 'powerbi_dataset'\n",
        "    pbidata_ent_qn = pbi_workspace + '/datasets/' + pbi_datasetid \n",
        "    purview_basepath = 'pyapacheatlas://'\n",
        "    #\"https://msit.powerbi.com/groups/7d666287-f9b8-45ff-be6c-9909afe9df40/datasets/e5a30c22-466d-4a30-a1ac-8736ed6567cc\"\n",
        "\n",
        "    pbidata_ent = AtlasEntity(\n",
        "        name=pbidata_ent_name,\n",
        "        typeName=pbidata_entity_type,\n",
        "        qualified_name= pbidata_ent_qn,\n",
        "        workspace = pbi_workspace,\n",
        "        guid = guid.get_guid()\n",
        "    )\n",
        "\n",
        "    # Prepare all the entities as a batch to be uploaded.\n",
        "    batch = [pbidata_ent]\n",
        "    batch\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=batch)\n",
        "\n",
        "    #cretae powerbi_dataset_process lineage\n",
        "    in_ent_guids = []\n",
        "    in_ent_guid = get_entity_guid(purview_basepath + ml_dataset_ent_name,ml_dataset_ent_type)\n",
        "    in_ent_guids.append({'guid':in_ent_guid})\n",
        "\n",
        "    out_ent_guids = []\n",
        "    out_ent_guid = get_entity_guid(pbidata_ent_qn,pbidata_entity_type)\n",
        "    out_ent_guids.append({'guid':out_ent_guid})\n",
        "\n",
        "    process_name =  'createpowerbidataset' + pbidata_ent_name + experiment_name\n",
        "    process_qn = \"pyapacheatlas://\" + process_name\n",
        "    process_type_name = \"powerbi_dataset_process\"\n",
        "\n",
        "    process = AtlasProcess(\n",
        "        name=process_name,\n",
        "        typeName=process_type_name,\n",
        "        qualified_name=process_qn,\n",
        "        inputs = in_ent_guids,\n",
        "        outputs = out_ent_guids,\n",
        "        guid=guid.get_guid()\n",
        "    )\n",
        "\n",
        "    # Prepare all the entities as a batch to be uploaded.\n",
        "    batch = [process]\n",
        "    batch\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=batch)\n",
        "    \n",
        "def create_powerbi_report_and_lineage(experiment_name,pbi_workspace,pbi_reportid,pbi_ent_name,pbi_datasetid):\n",
        "\n",
        "    #create powerbi report\n",
        "    pbi_entity_type = 'powerbi_report'\n",
        "    pbi_ent_qn = pbi_workspace + '/reports/' + pbi_reportid \n",
        "    purview_basepath = 'pyapacheatlas://'\n",
        "    \n",
        "    pbi_ent = AtlasEntity(\n",
        "        name=pbi_ent_name,\n",
        "        typeName=pbi_entity_type,\n",
        "        qualified_name= pbi_ent_qn, \n",
        "        workspace = pbi_workspace,\n",
        "        guid = guid.get_guid()\n",
        "    )\n",
        "\n",
        "    # Prepare all the entities as a batch to be uploaded.\n",
        "    batch = [pbi_ent]\n",
        "    batch\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=batch)\n",
        "\n",
        "    #create powerbi dashboard process lineage\n",
        "    pbidata_ent_qn = pbi_workspace + '/datasets/' + pbi_datasetid \n",
        "    in_ent_guids = []\n",
        "    in_ent_guid = get_entity_guid(pbidata_ent_qn,'powerbi_dataset')\n",
        "    in_ent_guids.append({'guid':in_ent_guid})\n",
        "\n",
        "    out_ent_guids = []\n",
        "    out_ent_guid = get_entity_guid(pbi_ent_qn,'powerbi_report')\n",
        "    out_ent_guids.append({'guid':out_ent_guid})\n",
        "\n",
        "    process_name = 'createpowerbireport' + pbi_ent_name + experiment_name\n",
        "    process_qn = \"pyapacheatlas://\" + process_name\n",
        "    process_type_name = \"powerbi_report_process\"\n",
        "\n",
        "    process = AtlasProcess(\n",
        "        name=process_name,\n",
        "        typeName=process_type_name,\n",
        "        qualified_name=process_qn,\n",
        "        inputs = in_ent_guids,\n",
        "        outputs = out_ent_guids,\n",
        "        guid=guid.get_guid()\n",
        "    )\n",
        "\n",
        "    # Prepare all the entities as a batch to be uploaded.\n",
        "    batch = [process]\n",
        "    batch\n",
        "\n",
        "    # Upload all entities!\n",
        "    client.upload_entities(batch=batch)\n",
        "    \n",
        "# clean up datasets\n",
        "def cleanup_entities(typename, entitytype):\n",
        "    filter_setup = {\"typeName\": typename, \"includeSubTypes\": True}\n",
        "    search = client.search_entities(\"*\", search_filter=filter_setup)\n",
        "    for entity in search:\n",
        "        #print(entity)\n",
        "        if entity.get(\"entityType\") == entitytype:\n",
        "            print(entity.get(\"id\"),entity.get(\"qualifiedName\"),entity.get(\"entityType\"))\n",
        "            guid = entity.get(\"id\")\n",
        "            client.delete_entity(guid=guid)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": true
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}