{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%run /01_Authenticate_to_Purview_AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%run /02_Create_ML_Lineage_Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "%run /03_Create_ML_Lineage_Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update below variables with synapse adls name and container/filesystem name\n",
    "data_lake_account_name = \"\"\n",
    "file_system_name = \"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "synapse_base_path = 'abfss://' + file_system_name + '@' + data_lake_account_name + '.dfs.core.windows.net'\n",
    "df_borrower = spark.read.load(synapse_base_path+ '/creditriskdata/borrower.csv', format='csv', header=True).toPandas()\n",
    "#display(df_borrower.head(10))\n",
    "\n",
    "df_loan = spark.read.load(synapse_base_path + '/creditriskdata/loan.csv', format='csv', header=True).toPandas()\n",
    "#display(df_loan.head(1))\n",
    "\n",
    "# Join data and do some transformations\n",
    "df_data = df_borrower.merge(df_loan,on='memberId',how='inner')\n",
    "df_data.shape\n",
    "\n",
    "df_sp = spark.createDataFrame(df_data)\n",
    "df_sp = df_sp.drop('loanStatus')\n",
    "\n",
    "df_sp.write.option('header', 'true').mode('overwrite').csv(synapse_base_path + '/creditriskdata/testdata/')\n",
    "\n",
    "df_data['homeOwnership'] = df_data['homeOwnership'].replace('nan', np.nan).fillna(0)\n",
    "df_data['isJointApplication'] = df_data['isJointApplication'].replace('nan', np.nan).fillna(0)\n",
    "\n",
    "drop_cols = ['memberId', 'loanId', 'date','grade']\n",
    "df_data = df_data.drop(drop_cols, axis=1)\n",
    "#df_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "experimentname = \"CreditRiskExperiment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create an entity for prepated data\n",
    "data_ent_name = 'creditriskdata'\n",
    "create_data_entity_with_schema(df_data,data_ent_name,'custom_dataset')\n",
    "\n",
    "#create preprocess lineage \n",
    "\n",
    "syn_basepath = 'https://' + data_lake_account_name + '.dfs.core.windows.net/' + file_system_name + '/creditriskdata'\n",
    "purview_basepath = 'pyapacheatlas://'\n",
    "\n",
    "in_ent_qns = {syn_basepath + '/borrower.csv':'azure_datalake_gen2_path',syn_basepath + '/loan.csv':'azure_datalake_gen2_path'}\n",
    "out_ent_qns = {purview_basepath + data_ent_name:'custom_dataset'}\n",
    "\n",
    "processname = '-preprocess'\n",
    "create_lineage_for_entities(experimentname,processname, in_ent_qns,out_ent_qns,ColumnMapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "\n",
    "##run only once\n",
    "experiment = Experiment(ws, experimentname)\n",
    "\n",
    "automl_classifier_config = AutoMLConfig(\n",
    "        task='classification', \n",
    "        enable_early_stopping = True, \n",
    "        iterations = 2,      \n",
    "        experiment_timeout_minutes=15,\n",
    "        primary_metric='AUC_weighted',\n",
    "        training_data= df_data,\n",
    "        #compute = 'local',\n",
    "        label_column_name='loanStatus',\n",
    "        n_cross_validations=5,\n",
    "        model_explainability=True,\n",
    "        enable_onnx_compatible_models=True,\n",
    "        enable_voting_ensemble=False,\n",
    "        enable_stack_ensemble=False\n",
    "        )\n",
    "local_run = experiment.submit(automl_classifier_config, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# get experiment run, get the best model and register\n",
    "\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "\n",
    "# get experiment run, get the best model and register\n",
    "experimentname = \"CreditRiskExperiment\"\n",
    "\n",
    "for automl_run in ws.experiments[experimentname].get_runs():\n",
    "    best_run, fitted_model = automl_run.get_output()  # We are taking the first run. You can update this if you like to take a different run\n",
    "    break\n",
    "\n",
    "#save the model to a local file\n",
    "model_path = 'creditrisk_model'\n",
    "joblib.dump(fitted_model, model_path)\n",
    "\n",
    "model_name = \"creditrisk_model\"\n",
    "registered_model = Model.register(model_path = model_path, # this points to a local file\n",
    "                       model_name = model_name, # name the model is registered as\n",
    "                       tags = {'type': \"classification\"}, \n",
    "                       description = \"Credit Risk Classifier\", \n",
    "                       workspace = ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create packages entities\n",
    "#[programming_language,package_name,version,notes]\n",
    "packageslist = [['python','mmlspark','v0.0.11','older versions before 0.0.10 give error'],\n",
    "                ['python','scikit-learn','0.22rc2.post1','latest version 0.24.x gives error if you call the model from Azure Function']]\n",
    "create_package_entities(experimentname,packageslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create experiment train lineage\n",
    "create_experiment_config_entity(ws,experimentname,automl_run)\n",
    "create_model_entity(ws,experimentname,model_name)\n",
    "create_model_metrics_entity(experimentname,best_run)\n",
    "\n",
    "pbasepath = 'pyapacheatlas://'\n",
    "\n",
    "in_ent_qns = {pbasepath + data_ent_name:'custom_dataset',pbasepath + experimentname + \"-config\":'custom_ml_exp_config',pbasepath + experimentname + '-packages':'custom_ml_packages'}\n",
    "out_ent_qns = {pbasepath + model_name:'custom_ml_model',pbasepath + experimentname + \"-modelmetrics\":'custom_ml_model_metrics'}\n",
    "\n",
    "processname = '-train'\n",
    "create_lineage_for_entities(experimentname,processname, in_ent_qns,out_ent_qns,ColumnMapping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "scoring_script = \"\"\"\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import azureml.train.automl\n",
    "from sklearn.externals import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # This name is model.id of model that we want to deploy deserialize the model file back\n",
    "    model_path = Model.get_model_path(model_name = 'creditrisk_model')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "def run(input_json):     \n",
    "    try:\n",
    "        data_df = pd.read_json(input_json)       \n",
    "        # Get the predictions...\n",
    "        prediction = model.predict(data_df)\n",
    "        prediction = json.dumps(prediction.tolist())\n",
    "    except Exception as e:\n",
    "        prediction = str(e)\n",
    "    return prediction\n",
    "\"\"\"\n",
    "exec(scoring_script)\n",
    "with open(\"scoring_script.py\", \"w\") as file:\n",
    "    file.write(scoring_script)\n",
    "    \n",
    "scoring_script_file_name = 'scoring_script.py'\n",
    "\n",
    "#test locally\n",
    "import numpy as np\n",
    "# X_test = spark.sql('select * from default.creditrisk_data limit 20').toPandas()\n",
    "drop_cols = ['loanStatus']\n",
    "X_test = df_data.drop(drop_cols, axis=1)\n",
    "X_test = X_test.head(1)\n",
    "json_test_data = X_test.to_json(orient='records')\n",
    "print(json_test_data)\n",
    "init()\n",
    "run(json_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# obtain conda dependencies from the automl run and save the file locally\n",
    "from azureml.core import Environment\n",
    "environment_config_file = 'creditrisk_conda_env.yml'\n",
    "best_run.download_file('outputs/conda_env_v_1_0_0.yml', environment_config_file)\n",
    "# with open('creditrisk_conda_env.yml', 'r') as f:\n",
    "#     print(f.read())\n",
    "\n",
    "# create the environment based on the saved conda dependencies file\n",
    "myenv = Environment.from_conda_specification(name=\"creditriskenv\", file_path=environment_config_file)\n",
    "myenv.register(workspace=ws)\n",
    "\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.webservice import Webservice\n",
    "\n",
    "# Configure and deploy the web service to Azure Container Instances\n",
    "inference_config = InferenceConfig(environment=myenv, entry_script=scoring_script_file_name)\n",
    "aci_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb= 2, tags = { 'type' : 'automl-classification'}, description='AutoML Credit Risk Classifier Service')\n",
    "aci_service_name = 'creditrisk-automl-service'\n",
    "aci_service = Model.deploy(ws, aci_service_name, [registered_model], inference_config, aci_config)\n",
    "aci_service.wait_for_deployment(show_output = True)\n",
    "print(aci_service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "aci_service_name = 'creditrisk-automl-service'\n",
    "create_model_service_entity(ws,experimentname,aci_service_name,json_test_data)\n",
    "\n",
    "pbasepath = 'pyapacheatlas://'\n",
    "\n",
    "in_ent_qns = {pbasepath + model_name:'custom_ml_model'}\n",
    "out_ent_qns = {pbasepath + experimentname + \"-model_endpoint\":'custom_ml_model_endpoint'}\n",
    "\n",
    "processname = '-deploymodel'\n",
    "create_lineage_for_entities(experimentname,processname, in_ent_qns,out_ent_qns,ColumnMapping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#batch inferencing\n",
    "df_test = spark.read.load(synapse_base_path +'/creditriskdata/testdata', format='csv', header=True).toPandas()\n",
    "\n",
    "drop_cols = ['memberId', 'loanId', 'date','grade']\n",
    "df_test1 = df_test.drop(drop_cols, axis=1)\n",
    "\n",
    "model_path = Model.get_model_path(model_name = 'creditrisk_model')\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "prediction = model.predict(df_test1)\n",
    "prediction\n",
    "\n",
    "df_result = df_test \n",
    "df_result['prediction'] = prediction\n",
    "df_result\n",
    "\n",
    "data_lake_account_name = 'purviewaccdl'\n",
    "file_system_name = 'purviewaccfs'\n",
    "df_sp = spark.createDataFrame(df_result)\n",
    "df_sp.write.option('header', 'true').mode('overwrite').csv(synapse_base_path + '/creditriskdata/batchpredictions/')\n",
    "\n",
    "df_sp.write.mode(\"overwrite\").saveAsTable(\"default.creditrisk_predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "#create an entity for test data\n",
    "test_data_ent_name = 'creditrisktestdata'\n",
    "create_data_entity_with_schema(df_test,test_data_ent_name,entitytype='custom_dataset')\n",
    "\n",
    "#create an entity for batch inference data\n",
    "batchpred_data_ent_name = 'creditriskbatchpredictions'\n",
    "create_data_entity_with_schema(df_result,batchpred_data_ent_name,entitytype='custom_dataset')\n",
    "\n",
    "#create batch inference lineage \n",
    "syn_basepath = 'https://' + data_lake_account_name + 'dfs.core.windows.net' + file_system_name + '/'\n",
    "pbasepath = 'pyapacheatlas://'\n",
    "\n",
    "in_ent_qns = {pbasepath + test_data_ent_name:'custom_dataset',pbasepath + model_name:'custom_ml_model'}\n",
    "out_ent_qns = {pbasepath + batchpred_data_ent_name:'custom_dataset'}\n",
    "\n",
    "processname = '-batchinference'\n",
    "create_lineage_for_entities(experimentname,processname, in_ent_qns,out_ent_qns,ColumnMapping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## uncomment below code to link PowerBI Dataset and Report in lineage if you have access to a PBI workspace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #The PowerBI entities will populate with more details if you set up a scan for PBI workspaces in Purview\n",
    "# #We are only creating placeholders and links for lineage below\n",
    "\n",
    "# #create PowerBI dataset entity and lineage \n",
    "# pbi_workspace = '<YOUR PBIWORKSPACE URL>' #'https://xxx.powerbi.com/groups/7c555287-f9b8-45ff-be6c-9909afe9df40'\n",
    "# pbi_datasetid = '<YOUR PBI Dataset ID>' #'c4a30c22-466d-4a30-a1ac-8736ed6567cc' \n",
    "\n",
    "# pbidata_ent_name = 'creditriskpbidataset' \n",
    "\n",
    "# create_powerbi_dataset_and_lineage(experimentname,pbi_workspace,pbi_datasetid,pbidata_ent_name,batchpred_data_ent_name,'custom_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #create PowerBI report entity and lineage\n",
    "# pbi_reportid = '<YOUR PBI Report ID>' #'e495453d-6c0c-4fb9-bdc4-556319f6a57b'\n",
    "# pbi_ent_name = 'creditriskpbireport'\n",
    " \n",
    "# create_powerbi_report_and_lineage(experimentname,pbi_workspace,pbi_reportid,pbi_ent_name,pbi_datasetid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "save_output": true
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
