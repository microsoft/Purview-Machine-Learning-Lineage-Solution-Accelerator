{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82095a0d",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft. All rights reserved.\n",
    "\n",
    "Licensed under the MIT license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "default_ds.upload_files(files=['./Data/borrower.csv', './Data/loan.csv'], # Upload the diabetes csv files in /data\n",
    "                       target_path='creditrisk-data/', # Put it in a folder path in the datastore\n",
    "                       overwrite=True, # Replace existing files of the same name\n",
    "                       show_progress=True)\n",
    "\n",
    "#Create a Tabular dataset from the path on the datastore\n",
    "from azureml.core import Dataset\n",
    "\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'creditrisk-data/borrower.csv'))\n",
    "\n",
    "tab_data_set = tab_data_set.register(workspace=ws,\n",
    "                                        name='BorrowerData',\n",
    "                                        description='Borrower Data',\n",
    "                                        tags = {'format':'CSV'},\n",
    "                                        create_new_version=True)\n",
    "\n",
    "#Create a Tabular dataset from the path on the datastore\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'creditrisk-data/loan.csv'))\n",
    "\n",
    "tab_data_set = tab_data_set.register(workspace=ws,\n",
    "                                        name='LoanData',\n",
    "                                        description='Loans Data',\n",
    "                                        tags = {'format':'CSV'},\n",
    "                                        create_new_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset, Datastore, ScriptRunConfig, Experiment\n",
    "from azureml.data.data_reference import DataReference\n",
    "import os\n",
    "import azureml.dataprep as dprep\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "borrowerData = Dataset.get_by_name(ws, name='BorrowerData')\n",
    "loanData = Dataset.get_by_name(ws, name='LoanData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "datastore = ws.get_default_datastore()\n",
    "\n",
    "# Create a compute cluster\n",
    "compute_name = 'cpu-cluster'\n",
    "if not compute_name in ws.compute_targets :\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS2_V2',\n",
    "                                                                min_nodes=0,\n",
    "                                                                max_nodes=1)\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    compute_target.wait_for_completion(\n",
    "        show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "    # Show the result\n",
    "    print(compute_target.get_status().serialize())\n",
    "\n",
    "compute_target = ws.compute_targets[compute_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "from azureml.core import Environment\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Create a Python environment for the experiment\n",
    "creditrisk_env = Environment(\"creditrisk-pipeline-env\")\n",
    "creditrisk_env.python.user_managed_dependencies = False # Let Azure ML manage dependencies\n",
    "creditrisk_env.docker.enabled = True # Use a docker container\n",
    "\n",
    "# Create a set of package dependencies\n",
    "creditrisk_packages = CondaDependencies.create(conda_packages=['scikit-learn','joblib','pandas','numpy','pip'],\n",
    "                                             pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\n",
    "\n",
    "\n",
    "# Add the dependencies to the environment\n",
    "creditrisk_env.python.conda_dependencies = creditrisk_packages\n",
    "\n",
    "# Register the environment \n",
    "creditrisk_env.register(workspace=ws)\n",
    "registered_env = Environment.get(ws, 'creditrisk-pipeline-env')\n",
    "\n",
    "# Create a new runconfig object for the pipeline\n",
    "aml_run_config = RunConfiguration()\n",
    "\n",
    "# Use the compute you created above. \n",
    "aml_run_config.target = compute_target\n",
    "\n",
    "# Assign the environment to the run configuration\n",
    "aml_run_config.environment = registered_env\n",
    "\n",
    "print (\"Run configuration created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile PrepareData.py\n",
    "from azureml.core import Run\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--prepared_data', dest='prepared_data', required=True)\n",
    "args = parser.parse_args()\n",
    "    \n",
    "borrowerData = Run.get_context().input_datasets['BorrowerData']\n",
    "loanData = Run.get_context().input_datasets['LoanData']\n",
    "\n",
    "df_borrower = borrowerData.to_pandas_dataframe()\n",
    "df_loan = loanData.to_pandas_dataframe()\n",
    "\n",
    "# Join data and do some transformations\n",
    "df_data = df_borrower.merge(df_loan,on='memberId',how='inner')\n",
    "df_data.shape\n",
    "\n",
    "df_data['homeOwnership'] = df_data['homeOwnership'].replace('nan', np.nan).fillna(0)\n",
    "df_data['isJointApplication'] = df_data['isJointApplication'].replace('nan', np.nan).fillna(0)\n",
    "\n",
    "drop_cols = ['memberId', 'loanId', 'date','grade','residentialState']\n",
    "df_data = df_data.drop(drop_cols, axis=1)\n",
    "\n",
    "df_data['loanStatus'] = np.where(df_data['loanStatus'] == 'Default', 1, 0) # change label column to 0/1\n",
    "\n",
    "df_data.to_csv(os.path.join(args.prepared_data,\"prepared_data.csv\"),index=False)\n",
    "\n",
    "print(f\"Wrote prepped data to {args.prepared_data}/prepared_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "prepared_data = OutputFileDatasetConfig(name=\"prepared_data\")\n",
    "\n",
    "dataprep_step = PythonScriptStep(\n",
    "    name=\"PrepareData\", \n",
    "    script_name=\"PrepareData.py\", \n",
    "    compute_target=compute_target, \n",
    "    runconfig=aml_run_config,\n",
    "    arguments=[\"--prepared_data\", prepared_data],\n",
    "    inputs=[borrowerData.as_named_input('BorrowerData'),loanData.as_named_input('LoanData')],\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepared_data = prepared_data_path.read_delimited_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile TrainTestDataSplit.py\n",
    "from azureml.core import Run\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--prepared_data', dest='prepared_data', required=True)\n",
    "parser.add_argument('--train_data', dest='train_data', required=True)\n",
    "parser.add_argument('--test_data', dest='test_data', required=True)\n",
    "args = parser.parse_args()\n",
    "\n",
    "df_data = pd.read_csv(args.prepared_data + '/prepared_data.csv')\n",
    "\n",
    "df_train=df_data.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "df_train=df_data.drop(df_train.index)\n",
    "\n",
    "df_train.to_csv(os.path.join(args.train_data,\"train_data.csv\"),index=False)\n",
    "df_train.to_csv(os.path.join(args.test_data,\"test_data.csv\"),index=False)\n",
    "\n",
    "print(f\"Wrote prepped data to {args.train_data}/train_data.csv\")\n",
    "print(f\"Wrote prepped data to {args.test_data}/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test train split the data \n",
    "train_data = OutputFileDatasetConfig(name=\"train_data\")\n",
    "test_data = OutputFileDatasetConfig(name=\"test_data\")\n",
    "\n",
    "test_train_step = PythonScriptStep(name = \"TestTrainDataSplit\",\n",
    "                                 script_name =\"TrainTestDataSplit.py\",\n",
    "                                 arguments = [\"--prepared_data\", prepared_data.as_input(),\n",
    "                                              \"--train_data\", train_data,\n",
    "                                              \"--test_data\", test_data],\n",
    "                                  outputs = [train_data,test_data],\n",
    "                                  compute_target = compute_target, \n",
    "                                  runconfig = aml_run_config, \n",
    "                                  allow_reuse = True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = train_data.read_delimited_files()\n",
    "training_data\n",
    "\n",
    "testing_data = test_data.read_delimited_files()\n",
    "testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile TrainModel.py\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import argparse\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def creditrisk_onehot_encoder(df_data):\n",
    "    catColumns = df_data.select_dtypes(['object']).columns\n",
    "    df_data[catColumns] = df_data[catColumns].fillna(value='Unknown')\n",
    "    \n",
    "    df_data = df_data.fillna(df_data.mean())\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols= pd.DataFrame(OH_encoder.fit_transform(df_data[catColumns]),columns = list(OH_encoder.get_feature_names(catColumns)))\n",
    "    \n",
    "    # Remove categorical columns (will replace with one-hot encoding)\n",
    "    numeric_cols = df_data.drop(catColumns, axis=1)\n",
    "    \n",
    "    # Add one-hot encoded columns to numerical features\n",
    "    df_result = pd.concat([numeric_cols, OH_cols], axis=1)\n",
    "    \n",
    "    # impute missing numeric values with mean\n",
    "    fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputed_df = pd.DataFrame(fill_NaN.fit_transform(df_result))\n",
    "    imputed_df.columns = df_result.columns\n",
    "    imputed_df.index = df_result.index\n",
    "    df_result = imputed_df\n",
    "\n",
    "    return(df_result)\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--train_data', dest='train_data', required=True)\n",
    "parser.add_argument('--test_data', dest='test_data', required=True)\n",
    "parser.add_argument('--metrics_data', dest='metrics_data', required=True)\n",
    "parser.add_argument('--model_data', dest='model_data', required=True)\n",
    "args = parser.parse_args()\n",
    "\n",
    "df_train = pd.read_csv(args.train_data + '/train_data.csv')\n",
    "df_test = pd.read_csv(args.test_data + '/test_data.csv')\n",
    "\n",
    "df_train = creditrisk_onehot_encoder(df_train)\n",
    "df_test = creditrisk_onehot_encoder(df_test)\n",
    "\n",
    "cols = [col for col in df_train.columns if col not in [\"loanStatus\"]]\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(df_train[cols].values, df_train[\"loanStatus\"].values)\n",
    "\n",
    "print('predicting ...')\n",
    "y_hat = clf.predict(df_test[cols].astype(int).values)\n",
    "\n",
    "acc = np.average(y_hat == df_test[\"loanStatus\"].values)\n",
    "print('Accuracy is', acc)\n",
    "\n",
    "print(\"save model\")\n",
    "os.makedirs('models', exist_ok=True)    \n",
    "joblib.dump(value=clf, filename= 'models/creditrisk_model.pkl')\n",
    "\n",
    "model = Model.register(model_path = 'models/creditrisk_model.pkl',\n",
    "                    model_name = 'creditrisk_model',\n",
    "                    description = 'creditrisk model',\n",
    "                    workspace = run.experiment.workspace,\n",
    "                    properties={'Accuracy': np.float(acc)})\n",
    "\n",
    "modeldata = []\n",
    "modeldata.append(('models/creditrisk_model.pkl','creditrisk_model'))\n",
    "df_model = pd.DataFrame(modeldata, columns=('modelfile', 'model_name'))\n",
    "\n",
    "metricsdata = []\n",
    "metricsdata.append(('Accuracy',acc))\n",
    "df_metrics = pd.DataFrame(metricsdata, columns=('Metric', 'Value'))\n",
    "\n",
    "df_model.to_csv(os.path.join(args.model_data,\"model_data.csv\"),index=False)\n",
    "df_metrics.to_csv(os.path.join(args.metrics_data,\"metrics_data.csv\"),index=False)\n",
    "\n",
    "print(f\"Wrote model data to {args.model_data}/model_data.csv\")\n",
    "print(f\"Wrote metrics data to {args.metrics_data}/metrics_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "model_data = OutputFileDatasetConfig(name=\"model_data\")\n",
    "metrics_data = OutputFileDatasetConfig(name=\"metrics_data\")\n",
    "\n",
    "train_step = PythonScriptStep(name = \"TrainModel\",\n",
    "                                 script_name =\"TrainModel.py\",\n",
    "                                 arguments = [\"--train_data\", train_data.as_input(),\n",
    "                                              \"--test_data\", test_data.as_input(),\n",
    "                                              \"--model_data\", model_data,\n",
    "                                              \"--metrics_data\", metrics_data],\n",
    "                                  outputs = [model_data,metrics_data],\n",
    "                                  compute_target = compute_target, \n",
    "                                  runconfig = aml_run_config, \n",
    "                                  allow_reuse = True\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile BatchInference.py\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "import joblib\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import argparse\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def creditrisk_onehot_encoder(df_data):\n",
    "    catColumns = df_data.select_dtypes(['object']).columns\n",
    "    df_data[catColumns] = df_data[catColumns].fillna(value='Unknown')\n",
    "    \n",
    "    df_data = df_data.fillna(df_data.mean())\n",
    "    OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    OH_cols= pd.DataFrame(OH_encoder.fit_transform(df_data[catColumns]),columns = list(OH_encoder.get_feature_names(catColumns)))\n",
    "    \n",
    "    # Remove categorical columns (will replace with one-hot encoding)\n",
    "    numeric_cols = df_data.drop(catColumns, axis=1)\n",
    "    \n",
    "    # Add one-hot encoded columns to numerical features\n",
    "    df_result = pd.concat([numeric_cols, OH_cols], axis=1)\n",
    "    \n",
    "    # impute missing numeric values with mean\n",
    "    fill_NaN = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imputed_df = pd.DataFrame(fill_NaN.fit_transform(df_result))\n",
    "    imputed_df.columns = df_result.columns\n",
    "    imputed_df.index = df_result.index\n",
    "    df_result = imputed_df\n",
    "\n",
    "    return(df_result)\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--test_data', dest=\"test_data\", type=str, required=True)\n",
    "parser.add_argument('--model_data', dest=\"model_data\", type=str, required=True)\n",
    "parser.add_argument('--batchinfer_data', dest='batchinfer_data', required=True)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "df_model = pd.read_csv(args.model_data + '/model_data.csv')\n",
    "# model_path = Model.get_model_path(model_name = 'best_model_data')\n",
    "model_name = df_model['model_name'][0]\n",
    "\n",
    "model_path = Model.get_model_path(model_name=model_name, _workspace=run.experiment.workspace)\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "df_test = pd.read_csv(args.test_data + '/test_data.csv')\n",
    "df_test = creditrisk_onehot_encoder(df_test)\n",
    "\n",
    "x_test = df_test.drop(['loanStatus'], axis=1)\n",
    "\n",
    "y_predict = model.predict(x_test)\n",
    "\n",
    "df_test['Prediction'] = y_predict\n",
    "\n",
    "df_test.to_csv(os.path.join(args.batchinfer_data,\"batchinfer_data.csv\"),index=False)\n",
    "\n",
    "print(f\"Wrote prediction data with to {args.batchinfer_data}/batchinfer_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data import OutputFileDatasetConfig\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "batchinfer_data = OutputFileDatasetConfig(name=\"batchinfer_data\").register_on_complete(name=\"CreditRiskBatchInferenceData\",description = 'Batch Inference Data Output')\n",
    "\n",
    "batchinfer_step = PythonScriptStep(\n",
    "    name=\"RunBatchInference\", \n",
    "    script_name=\"BatchInference.py\", \n",
    "    compute_target=compute_target, \n",
    "    runconfig=aml_run_config,\n",
    "    arguments=[\"--test_data\", test_data.as_input(),\"--model_data\", model_data.as_input(),\"--batchinfer_data\", batchinfer_data],\n",
    "    outputs = [batchinfer_data],\n",
    "    allow_reuse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "pipeline = Pipeline(ws, [dataprep_step, test_train_step, train_step,batchinfer_step])\n",
    "\n",
    "experiment = Experiment(workspace=ws, name='CreditRiskPipeline')\n",
    "\n",
    "run = experiment.submit(pipeline, show_output=True)\n",
    "run.wait_for_completion()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
